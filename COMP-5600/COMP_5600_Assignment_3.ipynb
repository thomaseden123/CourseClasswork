{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lIkgmsybYDBg",
        "outputId": "2518e6ef-7353-4aed-f8f0-5b58f863bf47"
      },
      "outputs": [],
      "source": [
        "# Imported libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Dataset (kagglehub)\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"PromptCloudHQ/imdb-data\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "files = os.listdir(path)\n",
        "print(\"Files in dataset directory:\", files)\n",
        "csv_file_path = os.path.join(path, 'IMDB-Movie-Data.csv')  # Replace with the actual filename\n",
        "data = pd.read_csv(csv_file_path)\n",
        "print(data.columns)\n",
        "print(data['Title'][1])\n",
        "print(data['Genre'][1])\n",
        "print(data['Description'][1])\n",
        "\n",
        "# Preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(stop_words)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = text.lower().split()\n",
        "    print(\"Original tokens:\", tokens)\n",
        "\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words and token.isalnum()]\n",
        "    print(\"Filtered tokens:\", filtered_tokens)\n",
        "\n",
        "    lemmatized = [lemmatizer.lemmatize(token, pos='n') for token in filtered_tokens]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "data['clean_desc'] = data['Description'].apply(preprocess_text)\n",
        "\n",
        "# Multi-label Labelling\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "genres = data['Genre'].apply(lambda x: x.split(','))\n",
        "print(genres)\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(genres)\n",
        "print(y[0])\n",
        "print(mlb.classes_)\n",
        "\n",
        "# Tokenize Descriptions\n",
        "text_description = data['clean_desc']\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_description)\n",
        "sequences = tokenizer.texts_to_sequences(text_description)\n",
        "word_index = tokenizer.word_index\n",
        "X = pad_sequences(sequences, maxlen=100)\n",
        "\n",
        "# Download Glove Embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "embedding_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "print('Found %s word vectors.' % len(embedding_index))\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    vector = embedding_index.get(word)\n",
        "    if vector is not None:\n",
        "        embedding_matrix[i] = vector\n",
        "\n",
        "# Train/Validation/Test Split\n",
        "X_train, X_rest, y_train, y_rest = train_test_split(X, y, train_size=700, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=200, random_state=42)\n",
        "\n",
        "# RNN Model\n",
        "print('\\n----- RNN Model -----')\n",
        "print('The Recurrent Neural Network (RNN) model uses a SimpleRNN layer to analyze movie descriptions '\n",
        "      'based on temporal and sequential data.')\n",
        "rnn_model = Sequential([\n",
        "    Embedding(input_dim=len(word_index)+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False),\n",
        "    SimpleRNN(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(20, activation='sigmoid')\n",
        "])\n",
        "rnn_model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "rnn_history = rnn_model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))\n",
        "\n",
        "# LSTM Model\n",
        "print('\\n----- LSTM Model -----')\n",
        "print('The Long Short-Term Memory (LSTM) model uses an LSTM layer to analyze movie descriptions '\n",
        "      'based on feedback connections and memory cells.')\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=len(word_index)+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False),\n",
        "    LSTM(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(20, activation='sigmoid')\n",
        "])\n",
        "lstm_model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_history = lstm_model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluation on Test Set\n",
        "print(\"RNN Test Performance:\", rnn_model.evaluate(X_test, y_test))\n",
        "print(\"LSTM Test Performance:\", lstm_model.evaluate(X_test, y_test))\n",
        "\n",
        "# Accuracy, Precision, Recall for RNN and LSTM\n",
        "def evaluate_multilabel_model(model, X_test, y_test, model_name=\"Model\"):\n",
        "    y_pred_prob = model.predict(X_test, verbose=0)\n",
        "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='micro', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "    print(f\"\\n{model_name} Evaluation Metrics:\")\n",
        "    print(f\"Accuracy:  {accuracy:.10f}\")\n",
        "    print(f\"Precision: {precision:.10f}\")\n",
        "    print(f\"Recall:    {recall:.10f}\")\n",
        "\n",
        "evaluate_multilabel_model(rnn_model, X_test, y_test, model_name=\"RNN\")\n",
        "evaluate_multilabel_model(lstm_model, X_test, y_test, model_name=\"LSTM\")\n",
        "\n",
        "# Plot Training History\n",
        "def plot_history(history, title):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    epoch1 = range(1, len(history.history['accuracy']) + 1)\n",
        "    epoch2 = range(1, len(history.history['loss']) + 1)\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epoch1, acc, label='Train Accuracy')\n",
        "    plt.plot(epoch1, val_acc, label='Validation Accuracy')\n",
        "    plt.title(f'{title} Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(epoch1)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epoch2, loss, label='Train Loss')\n",
        "    plt.plot(epoch2, val_loss, label='Validation Loss')\n",
        "    plt.title(f'{title} Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xticks(epoch2)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(rnn_history, \"RNN\")\n",
        "plot_history(lstm_history, \"LSTM\")\n",
        "\n",
        "# Summary of Part 1 in Assignment 3 - Deep Learning Applications\n",
        "\n",
        "# For this project, my goal was to implement two deep learning algorithms (RNN and LSTM) to sort out\n",
        "# IMDB movies from Kaggle into different genres based on their short descriptions. After the text was preprocessed\n",
        "# and lemmatized, I used Glove embeddings to tokenize each description. The genre labels were multi-labelled and\n",
        "# binarized using a MultiLabelBinarizer. Two models - the RNN and LSTM - were built using a SimpleRNN and a LSTM layer\n",
        "# along with embedding, dense, and dropout layers. These models were trained for 20 epochs each and evaluated by finding\n",
        "# accuracy, precision, and recall. The RNN had slightly better accuracy and recall, while the LSTM had slightly better precision.\n",
        "# Both of these models ended up struggling with accuracy, but they were both successful in demonstrating Multi-Label labelling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL1VBCqKnkcC"
      },
      "outputs": [],
      "source": [
        "# Imported libraries\n",
        "import kagglehub\n",
        "import os\n",
        "path = kagglehub.dataset_download(\"puneet6060/intel-image-classification\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "train_directory = os.path.join(path, 'seg_train/seg_train')\n",
        "test_directory = os.path.join(path, 'seg_test/seg_test')\n",
        "\n",
        "# Image Generators\n",
        "data = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = data.flow_from_directory(train_directory, target_size=(150,150), batch_size=128, class_mode='categorical')\n",
        "test_generator = data.flow_from_directory(test_directory, target_size=(150,150), batch_size=128, class_mode='categorical')\n",
        "\n",
        "# Model 1 - 3 Conv Layers, 3 Max Pooling Layers\n",
        "cnn_pt1 = Sequential([\n",
        "    Input(shape=(150,150,3)),\n",
        "    Conv2D(32, (3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "print('\\n----- Model 1 -----')\n",
        "cnn_pt1.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_pt1.fit(train_generator, validation_data=test_generator, epochs=5)\n",
        "\n",
        "# Model 2 - 6 Conv Layers, 3 Max Pooling Layers\n",
        "cnn_pt2 = Sequential([\n",
        "    Input(shape=(150,150,3)),\n",
        "    Conv2D(32, (3,3), activation='relu'),\n",
        "    Conv2D(32, (3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "print('\\n----- Model 2 -----')\n",
        "cnn_pt2.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_pt2.fit(train_generator, validation_data=test_generator, epochs=5)\n",
        "\n",
        "# Evaluated Accuracy\n",
        "print(\"Model 1 Accuracy:\", cnn_pt1.evaluate(test_generator))\n",
        "print(\"Model 2 Accuracy:\", cnn_pt2.evaluate(test_generator))\n",
        "\n",
        "# Per-Class Classification Report\n",
        "def evaluate_and_report(model, generator, model_name):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    class_names = list(test_generator.class_indices.keys())\n",
        "    for i in range(len(generator)):\n",
        "        x_batch, y_batch = generator[i]\n",
        "        pred = model.predict(x_batch, verbose=0)\n",
        "        y_true.extend(np.argmax(y_batch, axis=1))\n",
        "        y_pred.extend(np.argmax(pred, axis=1))\n",
        "\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "    df_report = pd.DataFrame(report).transpose()\n",
        "    print(f\"\\n--- {model_name} Classification Report ---\")\n",
        "    print(df_report)\n",
        "\n",
        "# Evaluate and Report Models\n",
        "evaluate_and_report(cnn_pt1, test_generator, \"Model 1\")\n",
        "evaluate_and_report(cnn_pt2, test_generator, \"Model 2\")\n",
        "\n",
        "# Visualize Predictions\n",
        "x_test, y_test = next(test_generator)\n",
        "pred1 = cnn_pt1.predict(x_test[:2], verbose=0)\n",
        "pred2 = cnn_pt2.predict(x_test[:2], verbose=0)\n",
        "class_names = list(test_generator.class_indices.keys())\n",
        "for i in range(2):\n",
        "    true_label = class_names[np.argmax(y_test[i])]\n",
        "    pred_label1 = class_names[np.argmax(pred1[i])]\n",
        "    pred_label2 = class_names[np.argmax(pred2[i])]\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.imshow(x_test[i])\n",
        "    plt.title(f\"Actual: {true_label}\\nModel 1 Prediction: {pred_label1} | Model 2 Prediction: {pred_label2}\", fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Summary of Part 2 in Assignment 3 - Deep Learning Applications\n",
        "\n",
        "# For this project, my goal was to implement a Convolutional Neural Network (CNN) to sort out natural images from the\n",
        "# Intel Kagglehub dataset into 6 classes. After these images were generated based on training and testing (validation),\n",
        "# two CNN models were built. The first CNN model was implemented using 3 Conv layers and 3 Max Pooling layers, while the second\n",
        "# model was implemented using 6 Conv layers and 3 Max Pooling layers. Both models included dropout and dense layers before adding\n",
        "# an output layer. These models were trained for 5 epochs each with rescaled image data. Model 2 demonstrated a stronger performance\n",
        "# than Model 1 in correctly classifying images due to its increased use of Conv layers."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
